## 정렬알고리즘
### 힙 정렬
  - 이진 힙(Binary Heap)
    - 힙 조건을 만족하는 완전 이진 트리
    - 힙 조건: 각 노드의 우선 순위가 자식 노드의 우선 순위보다 높다
    - 최대 힙: 가장 큰 값이 루트에 저장
    - 최소 힙: 가장 적은 값이 루트에 저장
    - n개의 노드를 가진 힙
      - 완전 이진 트리이므로, 힙의 높이가 log2n이며, 노드들을 빈 공간 없이 배열에 저장
      - ![image](https://user-images.githubusercontent.com/61530368/173304504-b157e6c4-4305-4136-af21-b8e4d296b6c6.png)
  - 힙의 노드 관계
    - 힙에서 부모와 자식의 관계
      - A[i]의 부모 = A[i/2]
        - 단, i가 홀수이면 i/2에서 정수 부분만
      - A[i]의 왼쪽 자식 = A[2i]
      - A[i]의 오른쪽 자식 = A[2i + 1]    
  - 힙 정렬
    - 정렬할 입력으로 최대 힙을 만든다
    - 힙 루트에 가장 큰 수가 있으므로, 루트와 힙의 가장 마지막 노드를 교환한다
      - 가장 큰 수를 배열의 맨 뒤로 옮긴 것
    - 힙 크기를 1개 줄인다
    - 루트에서 새로운 저장된 숫자로 인해 위배된 힙 조건을 해결하여 힙 조건을 만족시킨다
    - 이 과정을 반복하여 정렬한다 
  - 알고리즘
    ```
    입력: 입력이 A[1]부터 A[n]까지 저장된 배열 A
    출력: 정렬된 배열 A
    1. A의 숫자에 대해서 최대 힙을 만든다
    2. heapSize = n // 힙의 크기를 조절하는 변수
    3. for i = 1 to n - 1
    4.  A[1] <-> A[heapSize] //루트와 힙의 마지막 노드 교환
    5.  heapSize = heapSize = 1 // 힙의 크기 1 감소
    6.  DownHeap() // 위배된 힙 조건을 만족시킨다
    7. return A
    ```
   
  - DownHeap()
    - 루트로부터 자식들 중에서 큰 값을 가진 자식과 비교하여 힙 속성이 만족될 때까지 숫자를 교환하며 이파리 방향을 진행 
  - 시간 복잡도
    - O(n-1) * O(logn) = O(nlogn)
  - 알고리즘 수행 과정
    - ![image](https://user-images.githubusercontent.com/61530368/173306778-22428831-89f5-4a5e-b965-168ebab53828.png)
    - 힙의 마지막 노드 40과 루트 90을 바꾸고, 힙의 노드 수(heapSize) 1감소
    - ![image](https://user-images.githubusercontent.com/61530368/173307002-24ced9c2-d89b-446c-889f-abf3022af4fd.png)
    - 40은 다시 자식 노드 70과 10 중에서 큰 자식 70과 비교하여, 70과 40을 비교
    - ![image](https://user-images.githubusercontent.com/61530368/173307088-8511fc30-0b07-49d9-86b4-638cb0bb9d11.png)
    - 80 <-> 20
    - ![image](https://user-images.githubusercontent.com/61530368/173307174-f9d0edcb-841a-44d7-9f6b-de8c834a517f.png)
    - 70 <-> 10
    - ![image](https://user-images.githubusercontent.com/61530368/173307298-ea44929b-c3d2-4440-8b84-cd5f8ef2c63c.png)
    - 60 <-> 20
    - ![image](https://user-images.githubusercontent.com/61530368/173307400-f7a01de9-862f-4fe2-9cb2-89720e046232.png)
    - 50 <-> 20
    - ![image](https://user-images.githubusercontent.com/61530368/173307428-251127e6-174e-4cb6-9df7-4371b33125ba.png)
    - 40 <-> 10
    - ![image](https://user-images.githubusercontent.com/61530368/173307514-02c429e8-ed4a-410a-98c1-b6115e690025.png)
    - 30 <-> 20
    - ![image](https://user-images.githubusercontent.com/61530368/173307691-eeede18f-8881-4b3c-afba-b710806ec6f5.png)
    - 20 <-> 10
  - 힙 정렬의 특성
    - 큰 입력에 대해 DownHeap()을 수행할 때 자식을 찾아야 하므로 너무 많은 캐시 미스로 인해 페이지 부재를 야기시킴
    - 최선,최악,평균 시간 복잡도가 동일 O(nlogn)

  - 내부 정렬 알고리즘 성능 비교
    - ![image](https://user-images.githubusercontent.com/61530368/173308168-213bcdf2-33d0-4e18-8b1e-7d33a70f930b.png)
 

### 정렬 문제의 하한
  - 비교 정렬(Comparison Sort)
    - 버블 정렬, 선택 정렬, 삽입 정렬, 쉘 정렬, 힙 정렬, 합병정렬, 퀵 정렬의 공통점은 비교가 부분적이 아닌 숫자 대 숫자로 이뤄짐
  - 기수정렬은 비교정렬이 아님
    - 숫자들을 한 자리씩 부분적으로 비교 
  - 최댓값 찾는 문제의 하한
    - 최댓값을 찾기 위해 숫자들을 적어도 몇 번 비교해야 하는가?
    - 어떤 방식으로도 탐색하든지 적어도 (n-1)번의 비교가 필요
      - 왜냐하면 어떤 방식이라도 각 숫자를 적어도 한 번 비교해야
      - (n-1)보다 작은 비교 횟수가 의미하는 것은 n개의 숫자중에서 적어도 1개의 숫자는 비교되지 않았다는 것
      - 비교 안 된 숫자가 가장 큰 수일 수도 있기 때문에, (n-1)보다 적은 비교 횟수로는 최댓값을 항상 찾을 수는 없다  
  - 정렬 문제의 하한
    - 3개의 서로 다른 숫자 x, y, z에 대해서, 정렬에 필요한 모든 경우의 숫자 대 숫자 비교
    - ![image](https://user-images.githubusercontent.com/61530368/173327659-241f437c-6edb-4c17-b857-75888648ba70.png)
    - 각 내부 노드에서는 2개의 숫자가 비교
    - 비교 결과가 참이면 왼쪽으로 거짓이면 오른쪽으로 분기
    - 각 이파리에는 정렬된 결과 저장 
    - 리프노드의 갯수는 n!인데 이것으로 높이를 구할 수 있다 높이는 logn!
    - log(n!) = O(nlogn)이므로, 비교 정렬의 하한은 O(nlogn)
      - n! >= (n/2)^(n/2)이므로 log(n!) >= log(n/2)^(n/2) = (n/2)log(n/2) = O(nlogn)
    - 즉, O(nlogn)보다 빠른 시간 복잡도를 가진 비교 정렬 알고리즘은 존재하지 않는다  






### 기수정렬
  - 기수 정렬
    - 숫자를 부분적으로 비교하며 정렬
    - 기(radix)는 특정 진수를 나타내는 숫자들
      - 10진수의 기는 0,...,9
      - 2진수의 기는 0, 1
    - 기수 정렬은 제한적인 범위 내에 있는 숫자에 대해서 각 자릿수 별로 정렬하는 알고리즘 
    - ![image](https://user-images.githubusercontent.com/61530368/173278870-ad1e25d9-5cdb-48ab-9e45-970219c6ce75.png)
    - 안전성
      - 입력에 중복된 숫자가 있을 때, 정렬된 후에도 중복된 숫자의 순서가 입력에서의 순서와 동일하면 정렬 알고리즘이 안전성을 가진다고 한다 
  - 알고리즘
    - counting 방식
      ```
      입력: n개의 r진수의 k자리 숫자
      출력: 정렬된 숫자 
      1. for i = 1 to k
      2.  각 숫자의 i자리 숫자에 대해 안정한 정렬을 수행
      3. return A
      ```
    - LSD(Least Significant Digit) Radix Sort
      ```
      입력: n개의 r진수의 k자리 숫자(리스트로 입력)
      출력: 정렬된 숫자
      1. for i = 1 to k
      2.  각 숫자의 i자리 숫자 마다 빈 queue를 준비
      3.  리스트의 앞 숫자부터 i 자리 숫자 queue에 붙인다
      4.  숫자 queue를 순서대로 연결
      5. return A
      ```
      - O(k(r+n))
      - k가 logn보다 작으면 기존의 정렬 알고리즘보다 기수정렬이 좋음 
      - ![image](https://user-images.githubusercontent.com/61530368/173280401-d1f7026f-f007-4193-ad9d-580ce2c64b14.png)
    - MSD(Most Significant Digit) Radix sort
      - ![image](https://user-images.githubusercontent.com/61530368/173280536-9b2e252c-eea4-4222-9e2f-1d71b2d33366.png)
    - MSD가 LSD보다 더 보통 더 좋음
  - Radix Sort는 자릿수가 고정된것에서 사용하기가 좋음 (주민번호, 학번, 전화번호 등등..)  
### 외부정렬
  - 내부정렬
    - 입력이 주기억 장치 (내부 메모리)에 있는 상태에서 정렬이 수행되는 정렬
  - 외부 정렬
    - 입력 크기가 매우 커서 읽고 쓰는 시간이 오래 걸리는 보조 기억 장치에 입력을 저장할 수 밖에 없는 상태에서 수행되는 정렬
    - 주기억 장치의 용량이 1GB이고, 입력 크기가 100GB라면, 어떤 내부 정렬 알고리즘으로도 직접 정렬할 수 없다  
  - 주기억 장치에 수용할 만큼 Read/Sort
    - 외부 정렬은 입력을 분할하여 주기억 장치에 수용할 만큼의 데이터에 대해서만 내부 정렬을 수행하고, 그 결과를 보조기억 장치에 일단 다시 저장
      - 100GB의 데이터를 1GB 만큼씩 주기억 장치로 읽어 들이고, 퀵정렬과 같은 내부 정렬 알고리즘을 통해 정렬한 후, 다른 보조 기억장치에 저장
    - 이를 반복하면, 원래의 입력이 100개의 정렬된 블록으로 분할되어 보조 기억 장치에 저장  
    - ![image](https://user-images.githubusercontent.com/61530368/173282574-f8e2eb45-bb2d-4c1a-a1d4-47e6d4e04c4a.png)
  - 정렬된 블록의 합병
    - 정렬된 블록들을 반복적인 합병을 통해서 하나의 정렬된 거대한 블록으로 만든다
      - 블록들을 부분적으로 주기억 장치에 읽어 들여서, 합병을 수행하여 부분적으로 보조 기억 장치에 쓰는 과정 반복
    - 블록을 부분적으로 읽어 들인 상황
      - ![image](https://user-images.githubusercontent.com/61530368/173282796-c090b924-797e-44e8-bf54-551303f4e28c.png)
  - 1GB 블록 2개가 합병되는 과정 
    - ![image](https://user-images.githubusercontent.com/61530368/173283508-81233934-afbd-4716-a158-e6fdb2e519a0.png)
    - ![image](https://user-images.githubusercontent.com/61530368/173283528-073f4ba6-c47e-42a9-a65a-763cb50635b9.png)
    - 나머지 98개의 블록에 대해서 이와 같이 49회를 반복하면, 2GB 블록이 총 50개 만들어지고
    - 그 다음엔 2GB블록 2개씩 짝을 지워 합병하는 과정을 총 25회 수행하면, 4GB블록 25개가 만들어진다
    - 이러한 방식으로 계속 합병을 진행하면, 블록 크기가 2배로 커지고 블록의 수는 1/2로 줄어들게 되어 결국에는 100GB블록 1개만 남는다
  - 외부 정렬 알고리즘은 기억 장치에서의 읽고 쓰기를 최소화하는 것이 매우 중요
    - 외냐하면 보조 기억 장치의 접근 시간이 주기억 장치의 접근 시간보다 매우 오래걸리기 때문 
  - ExternalSort()
    - M = 주기억 장치의 용량
    - 외부 정렬 알고리즘은 입력이 저장된 보조 기억 장치 외에 별도의 보조 기억 장치 사용
    - 알고리즘에서 보조 기억장치는 'HDD'로 
  - 알고리즘
    ```
    입력: 입력 데이터 저장된 입력 HDD
    출력: 정렬된 데이터가 저장된 출력 HDD
    1. 입력 HDD에 저장된 입력을 M만큼씩 주기억 장치에 읽어 들인 후 내부 정렬 알고리즘으로 정렬하여 별도의 HDD에 저장한다. 다음 단계에서 별도의 HDD는 입력 HDD로 사용되고, 입력 HDD는 출력 HDD로 사용
    2. while 입력 HDD에 저장된 블록 수 > 1
    3.  입력 HDD에 저장된 블록을 2개씩 선택하여, 각각의 블록으로부터 데이터를 부분적으로 주기억 장치에 읽어 들여서 합병을 수행한다. 이때 합병된 결과는 출력 HDD에 저장한다. 단, 입력 HDD에 저장된 블록 수가 홀수일 때에는 마지막 블록은 그대로 출력 HDD에 저장
    4.  입려과 출력 HDD의 역할을 바꾼다
    5. return 출력 HDD
    ```
  - 시간 복잡도
    - 외부 정렬은 전체 데이터를 몇 번 처리(읽고 쓰기)하는가를 가지고 시간 복잡도를 측정
    - 패스(pass): 전체 데이터를 1회 처리하는 것
    - 외부 정렬 알고리즘에서는 line3에서 전체 데이터를 입력 HDD에서 읽고 합병하여 출력 HDD에 저장. 즉, 1패스가 수행된다
    - while-루프가 수행된 횟수가 알고리즘의 시간 복잡도
    - 입력 크기가 N이고, 메모리 크기가 M이라고 하면, line 3이 수행될 때마다 블록 크기가 2M, 4M, ..., 2^kM으로(2배)증가
    - 마지막에 만들어진 1개의 블록 크기가 2^kM이면 이 블록은 입력 전체가 합병된 결과를 가지므로, 2^kM=N
      - k는 while-루프가 수행된 횟수
      - 2^k = N/M
      - k = log2(N/M)
    - 시간복잡도: O(log(N/M))   
  - 2-way 합병
    - ExternalSort 알고리즘에서는 하나의 보조 기억 장치에서 2개의 블록을 동시에 주기억 장치로 읽어 들일 수 있다는 가정
    - 2개의 블록이 각각 다른 보조 기억 장치에서 읽어들여야 하는 경우도 있다
    - 테이프 드라이ㅡ




## NP 문제
### NP-완전문제
  - 문제 분류
    - 종류 
      - P, NP, NP-완전, NP-hard ...
    - 다항식 시간 복잡도를 가진 알고리즘으로 해결되는(polynomial) 문제 집합
      - 시간 복잡도가 O(logn), O(n), O(nlogn), O(n^2), O(n^3)등
      - 이러한 시간 복잡도는 점근적 표기법에 따르며 O(n^k)에 포함되기 때문. 단, k는 양의 상수
    - 다항식 시간보다 큰 복잡도를 가진 알고리즘으로 해결되는 문제의 집합
      - 여러 가지 문제 집합으로 다시 분류
      - 그 중에 가장 중요한 문제 집합은 지수 시간시간 복잡도를 가진 알고리즘으로 해결되는 NP-완전문제 집합 
    - NP-완전 문제의 특성
      - 어느 하나의 NP-완전 문제에 대해서 다항식 시간의 알고리즘을 찾아내면(즉, 다항식 시간에 해를 찾을 수 있으면) 모든 다른 NP-완전 문제도 다항식 시간에 해를 찾을 수 있다 
  - NP 문제 집합
    - NP문제 집합
      - P 문제 집합과 NP-완전 문제 집합을 둘 다 포함하는 문제의 집합
      - NP문제 집합에 속한 문제를 NP문제라고한다
      - NP문제는 비결정적 다항식 시간알고리즘을 가진 문제
    - NP 알고리즘
      - 비결정적 다항식 시간 알고리즘
      - 첫 번째 단계
        - 주어진 입력에 대해서 하나의 해를 추측하고
      - 두 번째 단계
        - 그 해를 다항식 시간에 확인한 후에
        - 그 해가 '맞다/아니다'라고 답한다
      - 해를 찾는 알고리즘이 아니라,
      - 해를 다항식 시간에 확인하는 알고리즘이다
  - 문제의 관계
    - P문제, NP-완전문제, NP문제 집합 사이의 관계
      - ![image](https://user-images.githubusercontent.com/61530368/173503835-5b9f3608-a392-4784-807b-89cb9927dc73.png)
    - P문제 집합이 NP문제 집합에 속하는 이유
      - P문제를 해결하는데 다항식 시간이 걸리므로 이를 NP알고리즘이 문제의 해를 다항식 시간에 확인하는 것과 대응시킬 수 있기 때문
      - P문제를 위한 NP 알고리즘은 해를 추측하는 단계를 생략하고, 해를 확인하는 단계 대신에 해를 직접 다항식 시간에 구하고 확인 결과를 '맞다'라고 답한다 
  - 문제의 변형 
    - NP 알고리즘은 추측한 해를 확인하여 '맞다/아니다'라고 답하므로, 문제의 해가 'yes' or 'no'가 되도록 주어진 문제를 변형시켜야한다
    - 이러한 유형의 문제를 결정문제라고 한다
    - 여행자문제(TSP)
      - 각 도시를 한 번씩만 방문하고 시작 도시로 돌아오는 최단 경로의 거리를 찾는 문제
      - 상수 K를 사용하여 다음과같이 결정 문제로 변형
        - 각 도시를 1번씩만 방문하고 시작 도시로 돌아오는 경로의 거리가 K보다 짧은 경로가 있는가 ?    
  - NP-완전 문제의 특성
    - NP-완전 문제의 특성을 알기 위해 어떤 문제를 다른 문제로 변환(reduction)하는 과정을 이해하여야 한다
    - 문제의 변환
      - 문제 A를 해결하기 위해서 문제 B를 해결하는 알고리즘을 이용하는 것을 의미
    - 문제의 변환 과정
      - 먼저 문제 A의 입력을 문제 B의 입력 형태(format)로 변환시키고,
      - 변환된 입력으로 문제 B를 해결하는 알고리즘을 수행
      - 마지막으로 수행 결과인 해를 문제 A의 해로 변환
      - ![image](https://user-images.githubusercontent.com/61530368/173505912-6169442a-4848-41d1-9174-280a42b9b3de.png)
  - 문제 변환 예시 
    - 문제 A = 부분 집합의 합(Subset Sum)문제
      - 정수의 집합 S에 대해, 부분 집합의 합 문제는 S의 부분 집합들중에서 원소의 합이 K가 되는 부분 집합을 찾는 문제
      - 예를들어, S={20, 35, 45, 70, 80}이고, K=105이라면, {35, 70}의 원소의 합이 105가 되므로, 문제의 해는{35, 70}
      - ![image](https://user-images.githubusercontent.com/61530368/173506211-2a272d4a-1636-4e8c-806c-9c15a687cfaf.png)
    - 문제 B = 분할(Partition) 문제
      - 정수의 집합 S에 대해, S를 분할하여 원소들의 합이 같은 2개의 부분 집합을 찾는 문제
      - 예를 들어, S={20, 35, 45, 70, 80}이 주어지면, X = {20, 35, 70}과 Y = {45, 80}이 해
      - 왜냐하면 X의 원소의 합이 20+35+70 = 125이고, Y의 원소의 합도 45+80 = 125이기 때문
      - ![image](https://user-images.githubusercontent.com/61530368/173506569-2396e5c0-e346-47b6-9f14-7fe58e4bf307.png)
    - 변환 아이디어
      - 부분 집합의 합 문제의 입력인 집합 S를 분할 문제의 입력으로 변환할 때 t를 집합 S에 추가
        - t = s - 2K
      - 단, s는 집합 S의 모든 원소의 합
      - 부분 집합의 합 문제를 해결하기 위해서, 집합 S' = S 합집합 {t]를 입력으로 하는 분할 문제를 위한 알고리즘을 이용 
      - 분할 문제 알고리즘의 해인 2개의 집합 X와 Y에 대해, X에 속한 원소의 합과 Y에 속한 원소의 합이 같으므로, 각각의 합은 (s-K)
      - 왜냐하면 새 집합 S'의 모든 원소의 합이 S+t = s +(s-2K) = 2s-2K이고, (2s-2K)의 1/2이면 (s-K)이므로 
      - 따라서 분할 문제의 해인 X와 Y 중에서 t를 가진 집합에서 t를 제거한 집합이 부분 집합의 합 문제의 해가 된다 
      - 왜냐하면 만일 X에 t가 속해 있었다면, X에서 t를 제외한 원소의 합이 (s-K)-t = (s-K)-(s-2K) = s-K-s+2K = K가 되기 때문
      - 그러므로 부분 집합의 합 문제의 해는 바로 X-t이다 
    - 예제 
      - 부분 집합의 합 문제를 분할 문제로 변환하여 해결하는 예제로 s, K, t값은 다음과 같다
      - s = 20 + 35 + 45 + 70 + 80 = 250
      - K =105
      - t = s-2K = 250-(2*105) = 250 - 210 = 40
      - ![image](https://user-images.githubusercontent.com/61530368/173509498-7941470b-4fb8-4ee4-99bf-4d2d023a4a13.png)
    - 시간복잡도
      - ![image](https://user-images.githubusercontent.com/61530368/173509728-308026e4-6817-4faa-9281-b44949aec757.png)
  - 추이 관계
    - 문제 A와 문제 B 사이에 다항식 시간 변환 관계가 성립하면, 문제 A가 문제 B로 다항식 시간에 변환이 가능하다고 한다
    - 동시에 문제 B가 문제 C로 다항식 시간에 변환가능하면, 결국 문제 A가 문제 C로 다항식 시간에 변환 가능하다
    - 이러한 추이 관계로 NP-완전 문제들이 서로 얽혀있어서, NP-완전 문제들 중에서 어느 한 문제만 다항식 시간에 해결되면, 모든 다른 NP-완전 문제들이 다항식 시간에 해결된다 
  - NP-하드 문제 집합
    - 문제의 변환을 통해 또 다른 문제 집합인 NP-하드 문제 집합을 다음과 같이 정의
      - 어느 문제 A  에 대해서, 만일 모든 NP 문제가 문제 A로 다항식 시간에 변환이 가능하다면, 문제 A는 NP-하드문제이다
    - '하드'란 적어도 어떤 NP문제보다 해결하기 어렵다는 뜻
    - 모든 NP 문제가 NP-하드 문제로 다항식 시간에 변환가능하여야 함에도 불구하고, NP-하드 문제는 반드시 NP문제일 필요는 없다
    - ![image](https://user-images.githubusercontent.com/61530368/173511763-44f01378-1238-4da0-8cbd-ee02d8843bb0.png)

    - NP-완전 문제의 정의 
      - 문제 A가 NP-완전 문제가 되려면,
      - 문제 A는 NP문제이고, 동시에
      - 문제 A는 NP-하드문제이다 
  


  - 정리
    - p문제
      - 문제가 주어지면 t시간 복잡도를 가지는 알고리즘이 있으면 P 문제
    - NP문제
      - 이 문제하고 추축흔 답이 주어지면 그 답이 문제에 대한 해답인지 t타임에 알아낼 수 있는 알고리즘이 있으면 NP문제 
    - NP완전문제  
### 근사 알고리즘
  - 근사알고리즘 
    - 근사 알고리즘은 근사해를 찾는 대신에 다항식 시간의 복잡도를 가진다
    - 근사 알고리즘은 근사해가 얼마나 최적해에 가까운지를 나타내는 근사비율을 알고리즘과 함께 제시해야함
    - 근사 비율은 근사해의 값과 최적해의 값의 비율로서, 1.0에 가까울수록 정확도가 높은 알고리즘
    - 근사 비율을 계산하려면 최적해를 알아야 하는 모순 발생
    - 최적해를 대신할 수 있는 '간접적인' 최적해를 찾고, 이를 최적해로 삼아서 근사 비율을 계산 
### 여행자 문제(TSP)
   - 조건
      - 도시 A에서 도시 B로 가는 거리는 도시 B에서 도시 A로 가는 거리와 같다(대칭성) - 이 조건은 MST를 사용할때 이렇게하면 쉬우어짐
      - 도시 A에서 도시 B로 가는 거리는 도시 A에서 다른 도시 C를 경유하여 도시 B로 가는 거리보다 짧다(삼각 부등식 특성) - 이 조건은 근사 비율을 구할 때 이렇게 하면 쉬워짐
    - MST는 모든 점을 사이클 없이 연결하는 트리 중에서 트리 간선의 가중치 합이 최소인 트리 (최적의 해보다 더 아래에 있는게 MST)
  - MST를 활용한 근사해 찾는 과정
    - MST를 활용하여 여행자 문제의 근사해를 찾기위해 삼각 부등식 원리를 적용
    - ![image](https://user-images.githubusercontent.com/61530368/173257069-066ffbe8-bdb9-43fc-9c91-9ec2f7eee1fd.png)
  - 알고리즘
    ```
    입력: n개의 도시, 각 도시간의 거리
    출력: 출발 도시에서 각 도시를 1번씩만 방문하고 출발 도시로 돌아오는 도시 순서
    1. 입력에 대하여 MST를 찾는다
    2. MST에서 임의의 도시로부터 출발하여 트리의 간선을 따라서 모든 도시를 방문하고 다시 출발했던 도시로 돌아오는 도시 방문 순서를 찾는다
    3. return 이전 단계에서 찾는 도시 순서에서 중복되어 나타나는 도시를 제거한 도시 순서(단, 도시 순서의 가장 마지막의 출발 도시는 제거하지 않는다)
    ```
    - Line 1
      - 크루스탈 알고리즘: O(mlogm), m은 간선의 수
      - 프림 알고리즘: O(n^2), n은 점의 수
    - Line 2
      - 트리 간선을 따라서 도시 방문 순서를 찾는 데는 O(n) 시간
      - 왜냐하면 트리의 간선 수가 2(n-1)이기 때문
    - Lin2 3
      - 방문 순서를 따라가며 단순히 중복된 도시를 제거하므로 O(n)시간
    - 시간복잡도 : 크루스칼 또는 프림 알고리즘의 시간 복잡도 
  - 근사비율
    - 여행자 문제의 최적해를 실질적으로 알 수 없음
    - 간접적인 최적해인 MST 간선의 가중치의 합(M)을 최적해의 값으로 활용
      - 왜냐하면 실제의 최적해의 값이 M보다 항상 크기 때문
    - Appro_MST_TSP 알고리즘이 계산한 근사해의 값은 2M보다는 크지 않다
      - Line 2에서 MST의 간선을 따라서 도시 방문 순서를 찾을 때 각 간선이 정확히 2번 사용되었으므로 경로의 총 길이는 2M 
      - Line 3에서는 삼각 부등식의 원리를 이용하여 지름길로 도시 방문 순서를 만들기 때문에, 이전 도시 방문 순서에 따른 경로의 길이보다 새로운 도시 방문 순서에 따른 경로의 길이가 더 짧다
    - 따라서 이 알고리즘의 근사비율은 2M/M = 2.0 보다 크지 않다
      - 근사해의 값이 최적해의 값의 2배를 넘지 않는다 
### 정점 커버 문제
  - 정점 커버(Vertex Cover)
    - 주어진 그래프 G=(V, E)에서 각 간선의 양 끝점들중에서 적어도 하나의 끝점을 포함하는 점들의 집합들 중에서 최소 크기의 집합을 찾는 문제
  - 그래프의 모든 간선이 정점 커버에 속한 점에 인전해 있다
    - 정점 커버에 속한 점들로 그래프의 모든 간선을 커버하는 것  
  - 정점커버 예제
    - ![image](https://user-images.githubusercontent.com/61530368/173257876-7facb9fd-a830-4392-8dea-b4f0d8a6d456.png)
    - 위의 그래프에서 {1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {1}이 각각 정점 커버
    - {2} 또는 {3}은 정점 커버가 아니다 
    - 정점 커버 문제의 해는 {1}  
  - 집합 커버로 해결하기
    - 집합 커버(Set Cover) 문제
      - n개의 원소를 가진 집합 U가 있고,
      - U의 부분집합들을 원소로 하는 집합 F가 주어질 때,
      - F의 원소들인 집합들 중에서 어떤 집합들을 선택하여 합집합하면 U와 가게 되는가?
      - 집합 F에서 서낵하는 집합들의 수를 최소화하는 문제  
    - 정점 커버 문제의 입력 그래프를 집합 커버 문제의 입력으로 변환하여 SetCover 알고리즘으로 해를 찾아서, 그 해를 정점 커버의 해로 삼는다
  - 정점 커버 입력을 집합 커버 입력으로 변환
    - ![image](https://user-images.githubusercontent.com/61530368/173258065-96e11aef-dad2-44f8-a556-3a3c4cfa51b3.png)
  - 집합커버의 근사해 vs 쵲거해
    - ![image](https://user-images.githubusercontent.com/61530368/173258226-c69a5f07-c85d-4594-9c20-65c35e3e587d.png)
    - 집합 커버의 근사 비율은 Klonn
  - 집합 커버보다 작은 근사 비율
    - 선택한 간선(red)의 양 끝점에 인접한 모든 간선(점선)들은 양 끝점에 의해 커버된다
    - ![image](https://user-images.githubusercontent.com/61530368/173258310-60033a33-894c-4924-8b46-051be5e16a5e.png)
  - 극대 매칭
    - 매칭이란 각 간선의 양쪽 끝점들이 중복되지 않는 간선의 집합
    - 극대 매칭은 이미 선택된 간선에 기반을 두고 새로운 간선을 추가하려 해도 더 이상 추가할 수 없는 매칭을 말함
  - 극대 매칭을 이용하여 정점커버를 해결하자
    - 간선의 양 끝점이 이미 커버된 간선의 끝점이 아닐때에만 선택
  - 알고리즘
    ```
    입력: 그래프 G=(V, E)
    출력: 정점 커버
    1. 입력 그래프에서 극대 매칭 M을 찾는다
    2. return 매칭 M의 각선의 양 끝점들의 집합
    ```
  - Approx_Matching_VC
    - G에서 극대 매칭 M으로 간선 a,b,c,d,e,f 선택
    - 근사해는 간선 a, b, c, d, e, f의 양 끝점들
      - 근사해 12개
    - 최적해 7개
    - ![image](https://user-images.githubusercontent.com/61530368/173258550-8fffa883-143d-4055-81ce-db36d815378a.png)
  - 시간복잡도
    - 그래프에서 극대 매칭을 찾는 시간 복잡도와 동일
    - 극대 매칭을 찾기 위해 하나의 간선을 선택할 때
      - 양 끝점이 이미 선택된 간선의 끝점인지를 검사해야하므로 O(n)시간
    - 입력 그래프의 간선 수가 m이면, 각 간선에 대해서 O(n) 시간
    - 시간 복잡도는 O(n) * m = O(nm)  
  - 근사비율
    - 근사 비율을 계산하기 위해서 극대 매칭을 간접적인 최적해로 사용
      - 매칭에 있는 간선의 수를 최적해의 값으로 사용
      - 어떠한 정점 커버라도 극대 매칭에 있는 간선을 커버해야하기 때문
    - Approx_Matching_VC 알고리즘
      - 극대 매칭의 각 간선의 양쪽 끝점들의 집합을 정점 커버의 근사해로서 반환하므로, 근사해의 값은 극대 매칭의 간선수의 2배
      - 근사 비율 = (극대 매칭의 각 간선의 양 끝점들의 수)/(극대 매칭의 간선 수) = 2.0 
  
### 작업 스케줄링 문제
  - n개의 작업의 수행 시간 ti i= 1, ...., n, m개의 동일한 기계가 주어질 때 모든 작업이 가장 빨리 종료되도록 작업을 기계에 배정하는 문제
    - 단, 한 작업은 배정된 기계에서 연속적으로 수행되어야 한다
    - 또한 기계는 한 번에 하나의 작업만을 수행
  - 간단한 답은 그리디 방법으로 작업을 배정
    - 현재까지 배정된 작업에 대해서 가장 빨리 끝나는 기계에 새 작업을 배정 
  - 알고리즘
    ```
    입력: n개의 작업, 각 작업 수행 시간 ti, i = 1, 2,..., n, Mj, j = 1, 2, ..., m
    출력: 모든 작업이 종료된 시간
    1. for j = 1 to m
    2.  L[j] = 0 // L[j] = 기계 Mj에 배정된 마지막 작업의 종료 시간
    3. for i = 1 to n
    4.   min = 1
    5.   for j = 2to m // 가장 일찍 끝나는 기계를 찾기
    6.    if L<j] < L[min]
    7.      min = j
    8.   작업 i를 기계 Mmin에 배정
    9.   L[min] = L[min] + ti
    10. return 가장 늦게 작업 종료 시간
    ```
  - 수행과정
    - 작업의 수행시간이 [5,2,4,3,4,7,9,2,4,1]
    - 4개의 기계
    - ![image](https://user-images.githubusercontent.com/61530368/173259655-a623186b-d317-480d-a5c1-7993d5f7a430.png)
    - ![image](https://user-images.githubusercontent.com/61530368/173259696-6a3aec8c-a1e7-4bc1-80ca-aa4de4d12371.png)
    - 가장 늦게 끝나는 작업의 종류 시간인 13을 리턴
  - 시간 복잡도
    - n개의 작업을 하나씩 가장 빨리 끄나는 기계에 배정
    - 이러한 기계를 찾기 위해 알고리즘 line 5~7의 for~루프가 (m-1)번 수행
    - 모든 기계의 마지막 작업 종료 시간인 L[j]를 살펴보아야하므로 O(m)시간
    - n개의 작업을 배정해야하고,
    - line 10 에서 배열 L을 탐색해야하므로
    - n * O(m) + O(m) = O(nm)
  - 근사 비율
    - OPTo를 최적해의 종료시각이라고 하면
    - 간접적인 최적해 OPT는 다음과 가이 정의 
      - 전체 작업 시간의 합 / m 
    - 작업을 m개의 기계에 동일하게 할당하기는 어려우므로 다음이 성립
      - OPT < OPTo 
    - Approx_JobScheduling알고리즘의 근사해가 OPT'이고, 간접인 최적해가 OPT일때, 다음을 정명
      - OPT' <= 2OPT
    - 마지막으로 배정된 작업 i가 T붜 수행되면, 전체 작업이 T+ti에 종료되므로 OPT' = T + ti 
    - ![image](https://user-images.githubusercontent.com/61530368/173259957-e4f13759-8a96-4b3b-aa0f-fac9bcc8b2a8.png)
  - 근사 비율 계산하기
    - ![image](https://user-images.githubusercontent.com/61530368/173260013-835c7991-e0c0-49cb-b82c-b9aa1f027e40.png)
    - T'는 작업 i를 제외한 모든 작업의 수행 시간의 합을 기계의 수 m으로 나눈 값
      - T'는 작업 i를 제외한 작업들의 평균 종료 시간
    - 그러면 T <= T'이다
      - 작업 i가 배정된 (가장 늦게 끝나는) 기계를 제외한 모든 기계에 배정된 작업은 적어도 T 이후에 종료되기 때문 
    - T와 T'의 관계인, T<=T'를 이용한 OPT' <= 2OPT 증명
      - ![image](https://user-images.githubusercontent.com/61530368/173260141-55d6ea31-e6a7-4506-9a4f-1e49288cefae.png)
### 클러스터링 문제
  - 클러스터링 문제
    - 2차원 평면의 n개의 점들 간의 거리를 고려하여 k개의 그룹으로 나누자
    - ![image](https://user-images.githubusercontent.com/61530368/173260295-2e77c208-704a-4ee2-9069-a02b95a75b93.png)
    - 클러스터링 문제
      - n개의 점을 k개의 그룹으로 나누고 각 그룹의 중심이 되는 k개의 점을 선택하는 문제
      - 단, 가장 큰 반경을 가진 그룹의 직경이 최소가 되도록 k개의 점을 선택해야함 
  - 그리디 방법
    - k개의 센터 선택 방법
      - 1개씩 서낵
      - 임의 점을 첫 번째 센터로 선택
      - ![image](https://user-images.githubusercontent.com/61530368/173260499-0b043a93-0403-4525-845b-33253827dd8c.png)
  - 두 번째 센터는 어느 점이 좋을까?
    - ![image](https://user-images.githubusercontent.com/61530368/173275627-ed428136-aebc-449c-a157-c7c6586f40b0.png)
    - 두 개의 센터가 서로 가까이 있는 것보다 멀리 떨어져 있는 것이 좋다
  - 세 번째 센터는 ?
    - 첫 번째와 두 번째 센터 둘 다에서 가장 멀리 떨어진 점을 선택
    - ![image](https://user-images.githubusercontent.com/61530368/173275866-831fd6fe-b0b1-42e0-b224-62dbc67eec59.png)
  - 알고리즘
    ```
    입력: n개의 점 xi, i= 0, 1,..., n-1, 그룹의 수 k > 1
    출력: k개의 점의 그룹 및 각 그룹의 센터
    1. C[1] = r, 단, xr은 랜덤하게 선택
    2. for j = 2 to k
    3.  for i = 0 to n - 1
    4.    if xi != 센터
    5.      xi와 각 센터까지의 거리를 계산하여, xi와 가장 가까운 센터까지의 거리를 D[i]에 저장
    6.  C[j]=i, 단, i는 D의 가장 큰 원소의 인덱스이고, xi는 센터가 아니다
    7. 센터가 아닌 각 점 xi로부터 앞서 찾은 k개의 센터까지 거리를 각각 계산하고 그 중에 가장 짧은 거리의 센터를 찾는다 .이때 점 xi는 가장 가까운 센터의 그룹에 속하게 된다
    8. return C와 각 클러스터에 속한 점들의 리스트 
    ```
  - 시간복잡도
    - 내부 for-루프: 각 점에서 각 센터까지의 거리를 계산하므로 O(kn)시간
    - Line 6 최대값을 찾으므로 O(n)시간
    - 외부 for-루프는 (k-1)회 반복하므로 
      - O(1) + (k-1) * (O(kn) +O(n))
    - Line 7 센터가 아닌 각 점으로부터 k개의 센터까지의 거리를 각각 계산하면서 최솟값을 찾으므로 O(kn)시간
    -  O(1) + (k-1) * (O(kn) +O(n)) + O(kn) = O(k^2n)
  - 근사비율
    - OPTo: 최적해의 그룹 직경의 최대값
    - OPT: (=d) 근사 알고리즘으로 찾은 k+1번째 center와 가장 가까운 기존 center(1~k번째 center중 하나)와의 거리
    - OPT': 근사 알고리즘으로 구한 근사 해에서 그룹 직경의 최대값
    - 최적해가 만든 그룹 중에서 가장 큰 직경을 OPTo라고 하자
    - OPTo의 하한을 간접적으로 찾기 위해 알고리즘이 k개의 센터를 모두 찾고 나서 (k+1)번째 센터를 찾은 상황에서, 즉, k=4일 때, 1개의 센터 C5를 추가한 상황을 살펴보자
    - ![image](https://user-images.githubusercontent.com/61530368/173277534-88542a95-50ba-4b4d-b420-32dfe62203e8.png)
  - Approx_k_Clusters 알고리즘 실제 활용
    - 첫 센터를 랜덤하게 선택하므로 보다 나은 클러스터링을 위해 알고리즘을 여러 차례 수행하여 얻은 결과중에 best 클러스터링을 사용한다
    - 비정상적인 데이터(노이즈, outlier)에 취약한 성능을 보이므로 선처리를 통해 이들을 제거 후 사용해야 한다 









## 유전 알고리즘
  - 유전자 알고리즘
    - 다윈의 진화론으로부터 창안된 해 탐색 알고리즘 
    - '적자생존'의 개념을 최적화 문제를 해결하는데 적용
  - GA 사이클
    - ![image](https://user-images.githubusercontent.com/61530368/172890504-a56e0714-71dc-4c2d-9e1a-122c3715a8cc.png)
    - 후보해 집합 n개 선택
    - 적합도평가에서 적합도가 높으면 뽑힐 확률이 올라감
    - 후보해 선택에서 m개를 선택(m < n)
    - 교차/돌연변이 연산으로 n개가 다시 만들어짐
    - 최소값 >= 이전 최솟값 중지 (k만큼 기다리게 해서 이전 최솟값 보다 현재 최솟값이 커도 진행하기도함)
  - 슈도코드
    ``` 
      1. 초기후보해 집합 G0을 생성
      2. G0의 각 후보해를 평가
      3. t = 0
      4. repeat
      5.  Gt로부터 Gt+1을 생성 (후보해 선택 및 교치/돌연변이 연산으로 새로운 후보해 생성)
      6.  Gt+1의 각 후보해를 평가 (evaluation)
      7.  t = t + 1
      8. until 종료 조건이 만족될 때까지
      9. return Gt의 후보해 중에서 가장 우수한 해 
    ```
    - 여러 개의 해를 임의로 생성하여 이들을 초기 세대(generation) G0로 놓고
    - repeat-루프에서 현재 세대의 해로부터 다음 세대의 해를 생성해가며
    - 루프가 끝났을 때의 마지막 세대에서 가장 우수한 해를 반환
    - 이 해들은 repeat-루프의 반복적인 수행을 통해서 최적해 또는 최적해에 근접한 해가 될 수 있으므로 후보해라고 부른다
  - 후보해
    - TSP: 5개의 도시(A, B, C, D, E), 시작도시 = A
    - TSP는 시작 도시에서 출발하여 모든 다른 도시를 1번씩만 방문하고 시작 도시로 돌아와야 하므로, ABCDEA, ACDEBA 등이 후보해 
    - ![image](https://user-images.githubusercontent.com/61530368/172892236-1b092096-ba03-4571-895d-809e7198fae0.png)
  - 후보해의 수
    - 시작 도시를 제외한 4개의 도시를 일렬로 나열하는 방법의 수:(5-1)! =4! =24
    - n개의 도시의 후보해 수 = (n-1)!
  - 후보해의 평가
    - ABCDEA의 값 = 5 + 2 + 1+ 3 + 9 = 20
    - ![image](https://user-images.githubusercontent.com/61530368/172892661-3ce0f696-ae2a-45bc-af58-326188022a6f.png)
  - 적합도
    - 후보해의 값 = 후보해의 적합도(Fitness value)
    - 후보해 중에서 최적해의 값에 근접한 적합도를 가진 후보해를 '우수한' 해라고 부른다
  - GA 연산
    - 선택(selection) 연산
      - 현재 세대의 후보해 중에서 우수한 후보해를 선택하는 연산 
      - 현재 세대에 n개의 후보해가 있으면
      - 이렇게 선택된 후보해의 수는 m개로 유지(m < n)
      - 룰렛휠 방법
        - 각 후보해의 적합도에 비례하여 원반의 면적을 할당, 원반을 회전시키면서 원반이 멈추었을 때 핀이 가리키는 후보해를 선택 
        - 적자생존 개념을 모방한 것 면적이 넓은 후보해가 선택될 확률이 높다
        - ![image](https://user-images.githubusercontent.com/61530368/172893579-02d2221d-e092-46df-b1e8-39a38fd87cc1.png)
      - 토너먼트 선택
        - 1. 후보해 집합에서 k개의 후보해를 랜덤하게 선택
        - 2. 선택된 k개 중에서 가장 적합도가 우수한 해를 1개 선택
        - 3. 1~2의 과정을 m개의 우수한 해를 선택할 때까지 반복 
        - 이진 토너먼트 선택 k = 2
    - 교차(crossover) 연산
      - 1-점(point) 교차 연산
        - 랜덤하게 교차할 점을 선태한 후, 두 개의 후보해를 교차점을 기준으로 뒷부분을 서로 교환
        - ![image](https://user-images.githubusercontent.com/61530368/172894139-93ec13ea-9dd8-4e62-b636-1509ffeead93.png)
      - 교차 연산의 목적
        - 선택 연산을 통해서 얻은 우수한 후보해보다 우수한 후보해를 생성하기 위해
      - 교차율(crossover Rate)
        - 문제에 따라 교차 연산을 수행할 후보해의 수를 조절하는데, 이를 교차율이라고 한다
        - 일반적으로 교차율은 0.2 ~ 1.0 번위에서 정한다  

    - 돌연변이 연산 
      - 교차 연산 수행 후에 돌연변이 연산 수행
      - 아주 작은 확률로 후보해의 일부분을 임의로 변형
      - 이 확률을 돌연변이율이라고 하며 일반적으로 (1/PopSize) ~ (1/Length)의 범위에서 사용
        - PopSize란 모집단 크기 (Population Size)로서 한 세대의 후보해의 수 (m)
        - Length란 후보해를 이진 표현으로 했을 경우의 bit 수
      - 돌연변이가 수행된 후에 후보해의 적합도가 오히려 나빠질 수도 있음 
      - ![image](https://user-images.githubusercontent.com/61530368/172894679-56bdc790-c6aa-4c47-b996-ac354ffe8bdf.png)
      - 돌연변이 연산의 목적
        - 다음 세대에 돌연변이가 이루어진 후보해와 다른 후보해를 교차 연산함으로써 이후 세대에서 매우 우수한 후보해를 생성하기 위해
      - 돌연변이 연산 역할
        - 지역최적화에서 글로벌 최적화로 가기위함(나빠질 수도 있음)
        - ![image](https://user-images.githubusercontent.com/61530368/172894963-118e0156-ade1-44e1-8671-6a891652b0d3.png)
  - 종료 조건
    - 유전자 알고리즘이 항상 최적해를 찾는다는 보장이 없기 때문에 종료 조건이 일정하지 않다
      - 일반적으로 알고리즘을 수행시키면서 더 이상 우수한 해가 출현하지 않으면 알고리즘을 종료 
  - GeneticAlgorithm 수행과정
    - 다음의 2차 함수에 대해 유전 알고리즘으로 0 <= x <= 31 구간에서 최대값을 찾아보자 
      - f(X) = -x^2 + 38x + 80
    - 초기 세대를 구성하는 후보해들을 결정
      - 먼저 한 세대의 후보해 수를 4로 정하고, 0~31에서 랜덤하게 4개의 후보해인 1, 29, 3, 10을 선택하였다고 가정
    - 각 후보해의 적합도
      - f(1) = -(1)^2 + 38(1) + 80 = 117
      - f(29) = 341
      - f(3) = 185
      - f(10) = 360  
      - ![image](https://user-images.githubusercontent.com/61530368/173115383-5fd7e0c2-853e-4b09-b03f-31bccf8f08a8.png)
      - 초기 세대의 평균 적합도는 250.75
      - 초기 세대의 평균 적합도와 다음 세대의 평균 적합도를 비교하여 좋아졌는지 나빠졌는지 판단 가능
    - 선택 연산
      - 룰렛 휠 선택 방법으로 후보해 4는 2번 선택 ,후보해 2와 3은 각각 1번 선택, 후보해 1은 선택이 안되었다고 가정
    - 교차 연산
      - 후보해 4가 2개이므로, 후보해 2와 4를 짝짓고, 후보해 3과 4를 짝지어 아래와 아래와 같이 교차 연산을 수행
      - 단, 1점-교차 연산을 위해 아래와 같이 임의의 교차점이 선택되었다고 가정 
    - 돌연변이 연산
      - 교차 연산 후에 후보해 1의 왼쪽에서 두 번째 bit가 돌연변이가 되어 '1'에서 '0'으로 바뀌었다고 가정
      - 다른 후보해는 교차 연산 후와 동일
    - ![image](https://user-images.githubusercontent.com/61530368/173116230-e0c89f61-c5fc-4da8-bb78-8083c0f5765b.png)

    - 두 번째 세대의 후보해에 대한 적합도
      - 평균 적합도가 343.5로 첫 세대의 250.75보다 많이 향상됨
      - ![image](https://user-images.githubusercontent.com/61530368/173116431-63d4078d-aa3a-46e0-9e2e-8a8170d325df.png)
    - 알고리즘 종료
      - 충분한 세대를 거쳐 repeat-루프를 더 수행하여 후보해의 적합도가 변하지 않으면 알고리즘을 종료
      - 후보해 중에서 가장 적합도가 높은 후보해를 리턴
  - TSP를 위한 GeneticAlgorithm
    - 여행자 문제를 해결할 때 GeneticAlgorithm을 적용하기 위해 사용되는 2가지의 교차 연산
      - 2점 교차 연산
      - 사이클 교차 연산 (TSP는 경로자체가 다 나와야하고 한번씩만 나와야하는데 이것때문에 사이클 교차연산을 적용)
    - 여행자 문제의 후보해
      - 시작 도시부터 각 도시를 중복없이 나열하여 만들어진다
    - 2점 교차 연산
      - 임의의 2점을 정한 후, 가운데 부분을 서로 교환
      - 이후 중복되는 도시(점선 박스 내의 도시)를 현재 후보해에 없는 도시로 차례로 바꾼다
      - ![image](https://user-images.githubusercontent.com/61530368/173117607-94632728-c4db-48a2-9cb0-cba3667d853b.png)
      - 후보해 1에 대해 가운데 부분을 제외한 부분에 있는 H, B, A를 각각 C, D, E로 바꾸고
      - 후보해 2에 대해 가운데 부분을 제외한 부분에 있는 C, D, E를 각각 H, B, A로 바꾼다
    - 사이클 교차 연산
      - 후보해 1에서 임의의 도시 C를 선택한 후, C와 같은 위치에 있는 후보해 2의 도시 D와 바꾼다 (1)
      - 바꾼 후에는 후보해 1에는 C가 없고, D가 2개 존재
      - 이를 해결하기 위해 후보해 1에 원래부터 있었던 D를 후보해 2에 D와 같은 위치에 있는 G와 바꾼다 (2)
      - 이렇게 반복하여 C가 후보해 2로부터 후보해 1로 바뀌게 되면 교차 연산을 마친다
      - ![image](https://user-images.githubusercontent.com/61530368/173118439-d2fb674b-b5cc-42ef-a320-f1ac272e6dfb.png)
  - 다양한 실험 필요
    - 유전자 알고리즘은 대부분의 경우 실제로 적지 않은 실험이 필요
    - 주어진 문제에 대해서 모집단의 크기, 교차율, 돌연변이율 등과 같은 파라미터가 다양한 실험을 통해서 조절되어야
    - repeat-루프의 종료 조건도 실험을 통해서 결정할 수밖에 없다
    - 또한 다양한 선택 연산과 교차 연산 중에서 어떤 연산이 주어진 문제에 적절한지도 많은 실험을 통해서 결정해야
  - 유전자 알고리즘 특징
    - 문제의 최적해를 알 수 없고, 기존의 어느 알고리즘으로도 해결하기 어려운 경우에, 최적해에 가까운 해를 찾는데 매우 적절한 알고리즘
    - 유전자 알고리즘이 최적해를 반드시 찾는다는 보장은 없으나 대부분의 경우 매우 우수한 해를 찾는다
    - 반면, 시간이 매우 오래 걸린다

## 모의 담금질 (Simulated Annealing)
  - 모의 담금질 기법
    - 모의 담금질기법은 높은 온도에서 액체 상태인 물질이 온도가 점차 낮아지면서 결정체로 변하는 과정을 모방한 해 탐색 알고리즘 
  - 이웃해
    - 이러한 방식으로 해를 탐색하려면, 후보해에 대해 이웃하는 해(이웃해)를 정의하여야
    - 아래의 오른쪽 그림에서 각 지점은 후보해이고 아래쪽에 위치한 해가 위쪽에 있는 해보다 우수한 해이다.
    - 또한 2개의 후보해 사이의 화살표는 이 후보해들이 서로 이웃하는 관계임을 나타낸다
    - ![image](https://user-images.githubusercontent.com/61530368/173177622-02e1843e-4d4d-462e-ae92-2b8e1179c946.png)
  - 탐색 과정 
    - 높은 T에서의 초기 탐색은 최솟값을 찾는데도 불구하고 확률 개념을 도입하여 현재 해의 이웃해 중에서 현재해보다 "나쁜" 해로 (위 방향으로) 이동하는 자유로움을 보일 수 도 있다
    - T가 낮아지면서 점차 탐색은 아래 방향으로 항햔다
      - T가 낮아질수록 위 방향으로 이동하는 확률이 점차 작아진다
    - 그림에서 처음 도착한 골짜기(지역 최적해, local optimum)에서 더 이상 아래로 탐색할 수 없는 상태에 이르렀을 때 '운 좋게' 위 방향으로 탐색하다가 전역최적(global optimum)를 찾은 것을 보여준다
  - 모의 담금질 기법의 특성
    - 유전자 알고리즘과 마찬가지로 모의 담금질 기법도 항상 전역 최적해를 찾아준다는 보장은 없음
    - 모의 담금질 기법의 또 하나의 특징은 하나의 초기해로부터 탐색이 진행된다는 것 
    - 반면에 유전자 알고리즘은 여러 개의 후보해를 한 세대로 하여 탐색을 수행
  - 알고리즘
    ``` 
    1. 임의의 후보해 s를 선택
    2. 초기 T를 정한다
    3. repeat
    4.  for i = 1 to kt //kt는 T에서의 for-루프 반복 횟수
    5.    s의 이웃해 중에서 랜덤하게 하나의 해 s'를 선택
    6.    d = (s'의 값) - (s의 값)
    7.    if d < 0 //이웃해인 s'가 더 우수한 경우
    8.      s <- s'
    9.      else
    10.       q <- (0, 1)사이에서 램덤하게 선택한 수
    11.       if(p < q) s <- s' // p는 자유롭게 탐색할 확률
    12.  T <- alpha*T //1보다 작은 상수 alpha를 T에 곱하여 새로운 T를 계산
    13. until 종료 조건이 만족될 때까지
    14. return s       
    ```
    - Line 9 ~ 11
      - s'가 s보다 우수하지 않더라도 0~1사이에서 랜덤하게 선택한 수 q가 확률 p보다 작으면, s'가 현재 해인 s가 될 기회를 준다
      - 이 기회가 그림에서 최소값을 찾는데도 불구하고 위쪽에 위치한 이웃해로 탐색을 진행 
      - p는 자유롭게 탐색할 확률 
      - ![image](https://user-images.githubusercontent.com/61530368/173178170-dbf0e46a-2c53-4f0b-9e57-f2304b1367b6.png)
    - Line 12
      - T를 일정한 비율 alpha로 감소시킨다
      - 실제로 0.8 <= alpha <= 0.99 범위에서 미리 정한 냉각율 alpha(cooling ratio)를 T에 곱하여 새로운 T를 계싼
      - 일반적으로 0.99에 가까운 수로 선택 
  - 확률 p 조절
    - 모의 담금질 기법은 T가 높을 때부터 점점 낮아지는 것을 확률 p에 반영시켜서 초기에는 탐색이 자유롭다가 점점 규칙적이 되도록 한다
    - 확률 p는 T에 따라서 변해야함
      - T가 높을 땐, p를 크게 하고
      - T가 0이 되면, p를 0으로 만들어서 나쁜 이웃해 s'가 s가 되지 못하도록 한다
    - s'와 s의 값의 차이 d에 따라 p 조절
      - d 값이 크면, p를 작게 하고
      - d 값이 작으면, p를 크게 한다
    - 이렇게 하는 이유는 값의 차이가 큼에도 불구하고 p를 크게 하면 그 동안 탐색한 결과가 무시되어 랜덤하게 탐색하는 결과를 낳기 때문임  
  - 확률 p
    - 두 가지 요소를 종합한 확률 p 
      - ![image](https://user-images.githubusercontent.com/61530368/173178300-a0f2d7a8-f103-4c4b-80ce-76ace16fc3f8.png)
      - T는 큰 값에서 0까지 변하고, d는 s'와 s의 값의 차이

  - 이웃해 정의
    - TSP의 이웃 해 정의 3가지 예
      - 삽입
      - 교환
      - 반전
  - 삽입
    - 2개의 도시를 랜덤하게 선택한 후에, 두 번째 도시를 첫 번째 옆으로 옮기고, 두 도시 사이의 도시들은 오른쪽으로 1칸씩 이동 
    - 도시 B와 F가 랜덤하게 선택되었다면, F가 B의 바로 오른쪽으로 이동한 후, B와 F사이의 C, D, E를 각각 오른쪽으로 1칸씩 이동  
    - ![image](https://user-images.githubusercontent.com/61530368/173178360-7d751723-00b5-4327-9dae-d515859ba0e7.png)
  - 교환
    - 2개의 도시를 랜덤하게 선택한 후에, 그 도시들의 위치를 서로 바꿈
    - 도시 B와 F가 랜덤하게 선택되었다면, B와 F의 자리를 서로 바꿈
    - ![image](https://user-images.githubusercontent.com/61530368/173178414-a076a700-8d6e-45ea-88f6-f69c7531922a.png)
  - 반전
    - 2개의 도시를 랜덤하게 선택한 후에, 그 두 도시 사이의 도시를 역순으로 만든다. 단, 선택된 두 도시도 반전에 포함시킴
    - 도시 B와 E가 랜덤하게 선택되었다면, [B C D E]가 역순으로 [E D C B]로 바뀜
    - ![image](https://user-images.githubusercontent.com/61530368/173178472-e9de3bbd-eebb-4ad0-bb6d-ed4986b0c89c.png)















